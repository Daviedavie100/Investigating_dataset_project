{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Investigate a Dataset - [Database_soccer]\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "### Dataset Description \n",
    "\n",
    "> In this project 1, I willl be analysing ultimate soccer dataset, which is an open-source dataset in kaggle. The dataset is a one .sql file comprising seven tables, each with different(unique) but interrelated features. First, Country table has 11 European countries. Second, league table has 11 lead championship names. The country and league tables are related by their ID. Third, match table has over 25, 000 matches for different seasons as well as betting odds from upto 10 providers. The match table is also related to the previous tables by country_id. in the rows and 2 columns id and name\n",
    "I have check the shape of table to determine the nummber of rows and columns.\n",
    "\n",
    "\n",
    "### Question(s) for Analysis\n",
    "1. What teams improved the most over the time period? \n",
    "2. Which players had the most penalties? \n",
    "3. Which was the the most preferred leg for penalty-takers in 2016 among the players who scored more than the mean penalties in that year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for all of the packages to be used.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to create a function that would read csv and load for very dataset to a name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to load data\n",
    "path='C:/Users/Davie/Desktop/Data/'\n",
    "def load_data(name, table_name):\n",
    "    name=pd.read_csv(path + 'Database_Soccer/'+ table_name) # reads the csv file and stores in the dataframe name\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "displaying few lines of each dataset from the soccer database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=load_data('country', 'Country.csv')# country data table\n",
    "country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league=load_data('league', 'League.csv')# league data table\n",
    "league.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the league name above, the league name for Germany is confusing. Germany 1. could mean there are a number of German countries, so it should be change to Germany Bundesliga 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match=load_data('match', 'Match.csv')# match data table\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing data in match table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player=load_data('player', 'Player.csv')# player data table\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The birthday contains time at 00:00:00, which could be removed to contain only year, month and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_attr=load_data('player_attr', 'Player_Attributes.csv') # player attributes data table\n",
    "player_attr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time 00:00:00 can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team=load_data('team', 'Team.csv')# team data table\n",
    "team.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column team_fifa_api_id could insignificant because there is already team_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_attr=load_data('team_attr', 'Team_Attributes.csv')# team attributes data table\n",
    "team_attr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want again to develop a function that I will be using in inspecting the datasets for missing data, getting descriptive statistics, dimensions, and features' data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function for inspecting the datasets\n",
    "def wrangles (tbl_name):\n",
    "    \n",
    "    inf=tbl_name.info(); # inspecting data types and instances with missing data \n",
    "    dim=tbl_name.shape; # inspecting dimensions of the dataset\n",
    "    desc=tbl_name.describe(); # getting descriptive statistics\n",
    "        \n",
    "    return inf, dim, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country dataset\n",
    "wrangles(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need of getting the mean for ids, since the names are string we can just unique values and counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country.name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# league data\n",
    "wrangles(league)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need of getting the mean for ids, since the names are string we can just unique values and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league.name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match data\n",
    "wrangles(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing data for column home_player_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the date has been stored as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player data\n",
    "wrangles(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "birthday column stored as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player attributes data\n",
    "wrangles(player_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dates stored as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team data\n",
    "wrangles(team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data for team_fifa_api_id, though this might be insignificant since there is already team_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team attributes data\n",
    "wrangles(team_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data for buildUpPlayDribbling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Cleaning\n",
    "\n",
    "First, I want to create functions that will help me drop duplicates, merge two datasets, change data type, remove missing rows, drop unnecessary columns, then proceed to to merge the country data to that for league. \n",
    "I will correct the league name for Germany 1. Bundesliga to Germany Bundesliga 1.\n",
    "I will also change the name column for both the country data and league data, and also make the datafrmaes have the same dimensions and finally merge the two dataframes into country_league data using the country id as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to rename columns in a data frame\n",
    "def col_rename(col_renamed_data, col_old, col_new):\n",
    "    if len(col_old)==2: # checks if there are two columns to be renamed\n",
    "        col_renamed_data.rename(columns={col_old[0]:col_new[0], col_old[1]:col_new[1]}, inplace=True) # renames the two columns in the dataset\n",
    "    else:\n",
    "        col_renamed_data.rename(columns={col_old:col_new}, inplace=True) # renames if there is only one column to be renamed\n",
    "        \n",
    "    return col_renamed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function drop columns\n",
    "def drop_cols(drop_col_data, col_name):\n",
    "    drop_col_data.drop(col_name, axis=1, inplace=True) # removing columns\n",
    "        \n",
    "    return drop_col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to remove row missing values\n",
    "def remove_row_missing_values(na_data):\n",
    "    na_data.dropna(axis=0, how='any', inplace=True) # removing all rows with missing values\n",
    "        \n",
    "    return na_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to remove duplicate rows\n",
    "def remove_duplicates(dup_data, col):\n",
    "    if col=='':\n",
    "        dup_data.drop_duplicates(inplace=True) # remove all duplicate rows\n",
    "        \n",
    "    else:\n",
    "        dup_data.dropna(subset=[col], inplace=True) # removing rows based on column duplicate values\n",
    "        \n",
    "    return dup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to change date column from string to datetime\n",
    "def changed_type(changed_type_data, col_type):\n",
    "    changed_type_data[col_type]=changed_type_data[col_type].astype('str')  # converting to string\n",
    "    changed_type_data[col_type]=changed_type_data[col_type].str.extract(r'(\\d{4}-\\d{2}-\\d{2})') # extracting the date\n",
    "    changed_type_data[col_type] = pd.to_datetime(changed_type_data[col_type], format='%Y-%m-%d') # converting to datetime\n",
    "\n",
    "    return changed_type_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to filter some columns\n",
    "def filter_col(f_data, col):\n",
    "    df=f_data.filter(col) # filters the columns\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to to merge two data frames using inner because i dont want do keep duplicates\n",
    "def merging_data(data1, data2, on_col):\n",
    "    df=data1.merge(data2, on =on_col, how='inner')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting the league name for Germany 1. Bundesliga\n",
    "league.replace(to_replace='Germany 1. Bundesliga', value='Germany Bundesliga 1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing both id and name columns for the country data\n",
    "country=col_rename(country, col_old=['name', 'id'], col_new=['country_name', 'country_id']) \n",
    "country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the name column and id column  for the league data\n",
    "league=col_rename(league, col_old=['name', 'id'], col_new=['league_name', 'league_id']) \n",
    "league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two dataframes country and league\n",
    "country_league_info=merging_data(country, league, on_col ='country_id')\n",
    "country_league_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I want to merge the player data to player attributes data into player_info dataframe.\n",
    "I will use either the player_api_id or player_fifa_api_id as the keys and drop the id columns in both datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of player_api_id on payer and player attribute data\n",
    "player.player_api_id.nunique()==player_attr.player_api_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of  player_fifa_api_id on player and player attribute data\n",
    "player.player_fifa_api_id.nunique()==player_attr.player_fifa_api_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of unique values of the player_fifa_api_id are not the same in both dataframes, I will just use both of them as the key because both the ids might be important in merging this data with another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping id column from player dataframe\n",
    "drop_cols(player, col_name='id')\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping id column from player_attr dataframe\n",
    "drop_cols(player_attr, col_name='id')\n",
    "player_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates for player attribute data\n",
    "remove_duplicates(player_attr, col='')\n",
    "player_attr.sort_values(['player_api_id', 'date']) # sorting the players by id and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the two dataframes on player_api_id and player_fifa_api_id using inner join because i dont want the unmatched rows\n",
    "player_info=merging_data(player, player_attr, on_col =['player_api_id', 'player_fifa_api_id'])\n",
    "player_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now want o examine the player_info dataframe, check for missing values, dimensions of each column, data types of each column as well as duplcate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above uotput, there are several missing values, the data type for birthday and date are all strings. I have also noted that attacking_work_rate has the least number of rows hence maximum number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the uniques for the coulmn attacking_work_rate\n",
    "player_info.attacking_work_rate.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to drop all rows with nan and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all rows with missing data\n",
    "remove_row_missing_values(player_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicate rows\n",
    "remove_duplicates(player_info, col='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.attacking_work_rate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.defensive_work_rate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output in above two cells, no information has been provided from the data description about the meaning of None, norm, y, stoc, le, ornal, es, tocky, ean o, and the numbers 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9 . Howerver, from inspection, I realize that there is some association in that all the numbers 0-9 and o on the defensive_work_rate relates to None  on the attacking_work_rate. Also, the following  pairs also relate: norm-ornal, y-es, stoc-tocky, le-ean. These could be change or transformed if additional information is provided or simply be dropped from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacking_none=player_info.query('attacking_work_rate==\"None\"')\n",
    "attacking_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing length of numbers 0-9 and o in the defensive_work_rate relates to None in the attacking_work_rate\n",
    "\n",
    "attacking_none.attacking_work_rate.value_counts()==attacking_none.defensive_work_rate.value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to change the data types for birthday and date from string to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting birthday column into datetime\n",
    "changed_type(player_info, col_type='birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date column into datetime\n",
    "changed_type(player_info, col_type='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, I want merge team data to team attributes data into into team info dataframe, check data types, missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of team_api_id in team and team attribute data\n",
    "team.team_api_id.nunique()==team_attr.team_api_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of team_fifa_api_id in team and team attribute data\n",
    "team.team_fifa_api_id.nunique()==team_attr.team_fifa_api_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these outputs, it is clear that neither the team_api_id nor the team_fifa_api_id matches in the two datasets. I will therefore merge them on both the team_api_id and team_fifa_api_id as the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the id for team dataframe\n",
    "drop_cols(team, col_name='id').drop_duplicates(subset='team_api_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team.team_long_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to correct the following team names: '1. FC Köln', '1. FC Nürnberg', '1. FSV Mainz 05','1. FC Kaiserslautern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting some team names\n",
    "team.team_long_name.replace(to_replace=['1. FC Köln', '1. FC Nürnberg', '1. FSV Mainz 05','1. FC Kaiserslautern'], \n",
    "                            value=['FC Köln', 'FC Nürnberg', 'FSV Mainz 05','FC Kaiserslautern'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_attr.dropna(how='any').drop_duplicates(subset='team_api_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the id for team attributes data\n",
    "drop_cols(team_attr, col_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging team to the team attribute\n",
    "team_info=team.merge(team_attr, on =['team_api_id', 'team_fifa_api_id'], how='inner') #\n",
    "team_info['team_fifa_api_id']=team_info['team_fifa_api_id'].astype(int)\n",
    "team_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will populate the NaN in the buildUpPlayDribbling column with the mean of the column. And finally drop duplcates in the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the mean of buildUpPlayDribbling column\n",
    "#mean=team_info.buildUpPlayDribbling.mean()\n",
    "\n",
    "# filling the NaNs in the buildUpPlayDribbling by the mean\n",
    "#team_info['buildUpPlayDribbling']=team_info['buildUpPlayDribbling'].fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime\n",
    "changed_type(team_info, col_type='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicate rows in the final team info merged data\n",
    "team_info=team_info.copy()\n",
    "drop_cols(team_info, col_name=['team_fifa_api_id',])\n",
    "team_info.drop_duplicates(subset='team_api_id')\n",
    "team_info.dropna(how='any', inplace=True)\n",
    "#remove_duplicates(team_info, col='')\n",
    "team_info.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I to examine and merge match data, country_league, and team data to form march info dataframe. I will check for duplicates, missing values and correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of league_id unique values in the match and country_league_info dataframes\n",
    "match.league_id.nunique()==country_league_info.league_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of country_id unique values in the match and country_league_info dataframes\n",
    "match.country_id.nunique()==country_league_info.country_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will merge match and country_league_info dataframes on league_id and country_id, so that I retain all the info about ids. I want to drop all the columns containing odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in match.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to remove all the columns with CAPITAL LETTERS, odds columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime for the match data\n",
    "changed_type(match, col_type='date').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicate rows from match data\n",
    "remove_duplicates(match, col='').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns containg odds and the match id\n",
    "odd_cols = list(filter(lambda x: x.isupper(), match.columns))\n",
    "drop_cols(match, col_name=odd_cols)\n",
    "drop_cols(match, col_name='id')\n",
    "match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the contents of some columns\n",
    "```match.goal.value_counts() # goal```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two cells, I realised that the coulmns goal, shoton, shotoff, foulcommit, card, cross, corner and possession contains information related to the web page but not realistic data.Checking through the nested infomation, I can't really make sense out of it since even the website link to the discription is not loading. Instead of deleting the columns with such issues, I will instead drop the columns containing unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.columns\n",
    "match_info=filter_col(match, col=['country_id', 'league_id', 'season', 'stage', 'date', 'match_api_id', 'home_team_api_id', 'away_team_api_id', 'home_team_goal', 'away_team_goal'])\n",
    "\n",
    "match_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_info['results'] = np.where(\n",
    "    match_info['home_team_goal'] == match_info['away_team_goal'], 'draw',\n",
    "    np.where(\n",
    "        match_info['home_team_goal'] > match_info['away_team_goal'], 'home win', 'away win'\n",
    "    )\n",
    ")\n",
    "\n",
    "match_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data\n",
    "match_info=merging_data(country_league_info, match_info, on_col =['country_id', 'league_id'])\n",
    "match_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_info=match_info.drop(columns=['country_id', 'league_id'])\n",
    "match_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt home and away teams id_vars are columns not melted\n",
    "# creating columns not melted\n",
    "unmelted_columns= [x for x in match_info.columns if x not in ['home_team_api_id', 'away_team_api_id']]\n",
    "\n",
    "# melting home and away teams api ids\n",
    "melted_match=match_info.melt(id_vars=unmelted_columns, var_name='Location', value_name='Team')\n",
    "melted_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up or changing the location values\n",
    "melted_match['Location']=melted_match['Location'].replace({'home_team_api_id':'home', 'away_team_api_id':'away'})\n",
    "melted_df=melted_match.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging the team_info to the melted_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['team_long_name', 'team_short_name', 'date']\n",
    "team_info = team_info.drop(columns=exclude_columns).drop_duplicates(subset='team_api_id')\n",
    "\n",
    "team_info.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info.rename(columns={'team_api_id':'Team'}, inplace=True)\n",
    "team_s=team_info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "melted_match=pd.merge(melted_df, team_s, on ='Team', how='left')\n",
    "#melted_match.drop_duplicates(subset='match_api_id' , inplace=True)\n",
    "melted_match.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating team dictionary with team api ids as the key and a longname as the value\n",
    "team_dict=team.set_index('team_api_id')['team_long_name'].to_dict()\n",
    "team_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up team i.e. using team dictionary to replace team api ids\n",
    "melted_match['Team']=melted_match['Team'].map(team_dict)\n",
    "melted_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating goals column\n",
    "melted_match['goals']=np.where(melted_match['Location']=='home',melted_match['home_team_goal'], melted_match['away_team_goal'])\n",
    "melted_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match['pts'] = np.where(\n",
    "    melted_match['results'] == 'draw', 1, \n",
    "    np.where(\n",
    "        (melted_match['results'] == 'away win') & (melted_match['Location'] == 'home'), 0, \n",
    "        np.where(\n",
    "            (melted_match['results'] == 'home win') & (melted_match['Location'] == 'away'), 0, 3\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "melted_match.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match.sort_values(by=['match_api_id', 'season'])\n",
    "melted_match.drop_duplicates() #subset='match_api_id', inplace=True\n",
    "melted_match.dropna(how='any')\n",
    "melted_match.sort_values(by='match_api_id', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goals For for the home team\n",
    "melted_match['GF'] = np.where(\n",
    "    # Case 1\n",
    "    (melted_match['Location'] == 'home') & (melted_match['results'] == 'home win'),  \n",
    "    melted_match['goals'],  \n",
    "    np.where(\n",
    "        # Case 2\n",
    "        (melted_match['Location'] == 'home') & (melted_match['results'] == 'away win'),  \n",
    "        melted_match['goals'],  \n",
    "\n",
    "        np.where(\n",
    "            # Case 3\n",
    "            (melted_match['Location'] == 'away') & (melted_match['results'] == 'home win'),  \n",
    "            melted_match['goals'],  \n",
    "\n",
    "            np.where(\n",
    "                # Case 4\n",
    "                (melted_match['Location'] == 'home') & (melted_match['results'] == 'draw'),  \n",
    "                melted_match['goals'],  \n",
    "\n",
    "                np.where(\n",
    "                    # Case 5\n",
    "                    (melted_match['Location'] == 'away') & (melted_match['results'] == 'draw'),  \n",
    "                    melted_match['goals'],  \n",
    "\n",
    "                    np.where(\n",
    "                        # Case 6\n",
    "                        (melted_match['Location'] == 'away') & (melted_match['results'] == 'away win'),  \n",
    "                        melted_match['goals'], \n",
    "\n",
    "                        0  # If none of the conditions are met, GF is 0 for the home team\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "melted_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goals Against for the home team\n",
    "melted_match['GA'] = np.where(\n",
    "    # Case 1\n",
    "    (melted_match['Location'] == 'home') & (melted_match['results'] == 'home win'),  \n",
    "    melted_match['away_team_goal'], \n",
    "\n",
    "    np.where(\n",
    "        # Case 2 \n",
    "        (melted_match['Location'] == 'away') & (melted_match['results'] == 'home win'),  \n",
    "        melted_match['home_team_goal'],  \n",
    "        np.where(\n",
    "            # Case 3\n",
    "            (melted_match['Location'] == 'home') & (melted_match['results'] == 'draw'),  \n",
    "            melted_match['away_team_goal'],  \n",
    "            np.where(\n",
    "                # Case 4\n",
    "                (melted_match['Location'] == 'away') & (melted_match['results'] == 'draw'),  \n",
    "                melted_match['home_team_goal'], \n",
    "\n",
    "                np.where(\n",
    "                    # Case 5\n",
    "                    (melted_match['Location'] == 'away') & (melted_match['results'] == 'away win'),  \n",
    "                    melted_match['home_team_goal'],  \n",
    "\n",
    "                    np.where(\n",
    "                        # Case 6\n",
    "                        (melted_match['Location'] == 'away') & (melted_match['results'] == 'home win'),  \n",
    "                        melted_match['home_team_goal'],  \n",
    "\n",
    "                        np.where(\n",
    "                        # Case 7\n",
    "                        (melted_match['Location'] == 'home') & (melted_match['results'] == 'away win'),  \n",
    "                        melted_match['away_team_goal'],  \n",
    "\n",
    "                        0  # Default case (if none of the above conditions are met\n",
    "                        )\n",
    "\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "melted_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Goal Difference\n",
    "melted_match['GD'] = melted_match['GF'] - melted_match['GA']\n",
    "melted_match.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a function to plot different visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual(data, data2, visual_type):\n",
    "    if visual_type=='barh':\n",
    "        data.plot(kind='barh', rot=0, width=0.7, alpha=0.8, color='grey', figsize=[8,20] ) # i want to creat horizontal bars\n",
    "            \n",
    "    elif visual_type=='hist':\n",
    "        fig,vis=plt.subplots(figsize=[10,8])\n",
    "        vis.hist(data, alpha=0.8, bins=40) # creating a histogram\n",
    "        #plt.grid(axis='y', alpha=0.6) # grid\n",
    "            \n",
    "    elif visual_type=='boxplot':\n",
    "        fig,vis=plt.subplots(figsize=[8,4])\n",
    "        vis.boxplot(data, vert=0) # creating a box plot\n",
    "        #plt.grid(axis='x', alpha=0.6)\n",
    "                     \n",
    "    else:\n",
    "        fig, vis=plt.subplots(figsize=[10,8])\n",
    "        plt.scatter(x=data, y=data2, alpha=0.8, color='blue') # creating scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of goals in the seasons\n",
    "goals_08_to_16=melted_match.groupby(['season', 'Team'])['goals'].sum() \n",
    "# plot histogram\n",
    "plot_visual(data=goals_08_to_16, data2='', visual_type='hist')\n",
    "plt.title('HISTOGRAM')\n",
    "plt.xlabel('bins')\n",
    "plt.ylabel('No of goals scored in the season');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that the distribution of goals is right screwed. Further investigation can be shown on the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot box plot\n",
    "plot_visual(data=goals_08_to_16, data2='', visual_type='boxplot')\n",
    "plt.title('SEASON 2008/2009 - 2015/2016 BOXPLOT')\n",
    "plt.xlabel('No of goals scored');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is rght skewed but with severa outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 1 What is the league table for the 4 major leagues in europe during 2015/2016 season? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match['league_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the data\n",
    "columns_order = ['Pos', 'season', 'league_name', 'Club', 'MP', 'W', 'D', 'L', 'Pts', 'GF', 'GA', 'GD', 'avg_goals']\n",
    "filtered_columns= ['Pos', 'Club', 'MP', 'W', 'D', 'L', 'Pts', 'GF', 'GA', 'GD', 'avg_goals']\n",
    "\n",
    "def league_table(league):\n",
    "    # Filter the data for the given league and season\n",
    "    df_15_16 = (\n",
    "        melted_match\n",
    "        .query('season == \"2015/2016\" and league_name == @league')  \n",
    "        .groupby(['season', 'league_name', 'Team'], as_index=False)\n",
    "        .agg({\n",
    "            'match_api_id': 'count',  # Total matches played\n",
    "            'pts': [\n",
    "                lambda x: (x == 3).sum(),  # Wins\n",
    "                lambda x: (x == 1).sum(),  # Draws\n",
    "                lambda x: (x == 0).sum(),  # Losses\n",
    "                'sum'  # Total points\n",
    "            ],\n",
    "            'GF': 'sum',\n",
    "            'GA': 'sum',\n",
    "            'GD': 'sum',\n",
    "            'goals': 'mean',  # Average goals\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Flatten the column names\n",
    "    df_15_16.columns = [\n",
    "        'season', 'league_name', 'Club', 'MP', \n",
    "        'W', 'D', 'L', 'Pts', 'GF', 'GA', 'GD', 'avg_goals'\n",
    "    ]\n",
    "\n",
    "    # Sort the result by Pts and GD\n",
    "    df_15_16 = df_15_16.sort_values(by=['Pts', 'GD'], ascending=[False, False])\n",
    "\n",
    "    # Add a Rank column\n",
    "    df_15_16['Pos'] = df_15_16.reset_index().index + 1\n",
    "    # Reorder columns \n",
    "    df_15_16 = df_15_16[columns_order].reset_index()\n",
    "    # filter columns\n",
    "    df_15_16  = df_15_16[filtered_columns]\n",
    "\n",
    "\n",
    "    return df_15_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) England Premier League Table for 2015/2016 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_table(league=\"England Premier League\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) France Ligue 1 Table for 2015/2016 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_table(league=\"France Ligue 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Spain LIGA BBVA Table for 2015/2016 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_table(league=\"Spain LIGA BBVA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Italy Serie A Table for 2015/2016 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_table(league=\"Italy Serie A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving for Google Looker Studio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_looker = (\n",
    "    melted_match\n",
    "    #.query('season == \"2009/2010\" and league_name == \"England Premier League\"')  # Filter for the specified season and league\n",
    "    .groupby(['season', 'league_name', 'Team'], as_index=False)\n",
    "    .agg({\n",
    "        'match_api_id': 'count',  # Total matches played\n",
    "        'pts': [\n",
    "            lambda x: (x == 3).sum(),  # Wins\n",
    "            lambda x: (x == 1).sum(),  # Draws\n",
    "            lambda x: (x == 0).sum(),  # Losses\n",
    "            'sum'  # Total points\n",
    "        ],\n",
    "        'GF': 'sum',\n",
    "        'GA': 'sum',\n",
    "        'GD': 'sum',\n",
    "        'goals': 'mean',  # Average goals\n",
    "    })\n",
    ")\n",
    "\n",
    "# Flatten the column names\n",
    "df_google_looker.columns = [\n",
    "    'season', 'league_name', 'Club', 'MP', \n",
    "    'W', 'D', 'L', 'Pts', 'GF', 'GA', 'GD', 'avg_goals'\n",
    "]\n",
    "\n",
    "# Sort the result by total_pts\n",
    "df_google_looker = df_google_looker.sort_values(by=['Pts', 'GD'], ascending=[False, False])\n",
    "\n",
    "df_google_looker.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "df_google_looker.to_csv('league_table_for_lookerStudio.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 2 Which are the top 3 teams for each league in 2015/2016 season? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "top_3_teams = (\n",
    "    melted_match\n",
    "    .query('season==\"2015/2016\"')  \n",
    "    .groupby(['season', 'league_name', 'Team'], as_index=False)['pts'] \n",
    "    .sum()\n",
    "    .sort_values(by=['season', 'league_name', 'pts'], ascending=[True, True, False])\n",
    "    .groupby(['season', 'league_name'], as_index=False)\n",
    "    .head(3)\n",
    ")\n",
    "\n",
    "top_3_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "# Create a bar plot\n",
    "sns.barplot(data=top_3_teams, x='pts', y='Team', hue='league_name', dodge=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Top 3 Teams in Each League (2015/2016 Season)', fontsize=16)\n",
    "plt.xlabel('Points', fontsize=12)\n",
    "plt.ylabel('Teams', fontsize=12)\n",
    "\n",
    "# Add horizontal lines to separate leagues\n",
    "league_boundaries = top_3_teams.groupby('league_name')['Team'].count().cumsum().values[:-1]\n",
    "for boundary in league_boundaries:\n",
    "    plt.axhline(y=boundary - 0.5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(title='League', fontsize=10, title_fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 3 What teams improved the most over the time period? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use Year-Over-Year Improvements (YoY)\n",
    "- The teams played different number of matches. We normalize Points Based on Total Matches Played\n",
    "- Then divide the points by the total matches played in each season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total points for each team per season\n",
    "match_imp08_16=melted_match.query(\n",
    "    'season in [\"2008/2009\", \"2009/2010\", \"2010/2011\", \"2011/2012\", \"2012/2013\", \"2013/2014\", \"2014/2015\", \"2015/2016\"]'\n",
    "    ).groupby(\n",
    "        ['season', 'Team'])['pts'].sum().unstack('season')\n",
    "\n",
    "match_imp08_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total matches played for each team per season\n",
    "match_impp08_16=melted_match.query(\n",
    "    'season in [\"2008/2009\", \"2009/2010\", \"2010/2011\", \"2011/2012\", \"2012/2013\", \"2013/2014\", \"2014/2015\", \"2015/2016\"]'\n",
    "    ).groupby(\n",
    "        ['season', 'Team'])['match_api_id'].count().unstack('season')\n",
    "\n",
    "match_impp08_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide points by number of matches played\n",
    "df_normalized=match_imp08_16/match_impp08_16\n",
    "df_normalized.dropna(how='any', inplace=True)\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate YoY improvements\n",
    "\n",
    "'''# Difference between consecutive seasons\n",
    "yoy_improvements = df_normalized.diff(axis=1)  \n",
    "\n",
    "# Add a column: total YoY improvement\n",
    "yoy_improvements['Total_YoY_Improvement'] = yoy_improvements.sum(axis=1)\n",
    "\n",
    "# Team with the maximum YoY improvement\n",
    "most_consistent_team = yoy_improvements['Total_YoY_Improvement'].idxmax()\n",
    "max_yoy_improvement = yoy_improvements.loc[most_consistent_team, 'Total_YoY_Improvement']\n",
    "\n",
    "# Result\n",
    "print(f\"The most consistent improver is {most_consistent_team} with a total YoY improvement of {max_yoy_improvement} points.\");'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate YoY improvements\n",
    "df_normalized['Total_YoY_Improvement']=(\n",
    "    (df_normalized['2009/2010'] - df_normalized['2008/2009']) +\n",
    "    (df_normalized['2010/2011'] - df_normalized['2009/2010']) +\n",
    "    (df_normalized['2011/2012'] - df_normalized['2010/2011']) +\n",
    "    (df_normalized['2012/2013'] - df_normalized['2011/2012']) +\n",
    "    (df_normalized['2013/2014'] - df_normalized['2012/2013']) +\n",
    "    (df_normalized['2014/2015'] - df_normalized['2013/2014']) +\n",
    "    (df_normalized['2015/2016'] - df_normalized['2014/2015'])\n",
    "    )\n",
    "\n",
    "df_normalized[df_normalized['Total_YoY_Improvement']>0].reset_index().sort_values(by='Total_YoY_Improvement', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve\n",
    "df0=df_normalized[df_normalized['Total_YoY_Improvement']>0].reset_index().sort_values(by='Total_YoY_Improvement', ascending=False)\n",
    "# drop\n",
    "df00=df_normalized[df_normalized['Total_YoY_Improvement']<0].reset_index().sort_values(by='Total_YoY_Improvement', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# chart 1 improve\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, position 1\n",
    "sns.barplot(data=df0, y='Team', x='Total_YoY_Improvement',dodge=False, palette='viridis')\n",
    "plt.title('Most Improved Teams Accross Seasons', fontsize=16)\n",
    "plt.xlabel('Total_YoY_Improvement', fontsize=12)\n",
    "plt.ylabel('Teams', fontsize=12)\n",
    "\n",
    "# chart 2 drop\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, position 2\n",
    "sns.barplot(data=df00, y='Team', x='Total_YoY_Improvement',dodge=False, palette='viridis_r')\n",
    "plt.title('Teams with Most Decline Across Seasons', fontsize=16)\n",
    "plt.xlabel('Total_YoY_Improvement', fontsize=12)\n",
    "plt.ylabel('') # remove duplicate axes plt.ylabel('Teams', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most consistent improver is Napoli with a total YoY improvement of 0.9473684210526316 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 1: Team Playing Style Analysis\n",
    "- Identify unique playing styles for teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "\n",
    "# Team attributes against goals and points obtained\n",
    "class_list =[]\n",
    "for i in melted_match.columns:\n",
    "    if 'Class' not in i:\n",
    "        class_list.append(i)\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list=['country_name', 'stage', 'date', 'match_api_id', 'home_team_goal', 'away_team_goal', 'results', 'Location']\n",
    "req_columns=[\n",
    " 'season',\n",
    " 'league_name',\n",
    " 'Team',\n",
    " 'buildUpPlaySpeed',\n",
    " 'buildUpPlayDribbling',\n",
    " 'buildUpPlayPassing',\n",
    " 'chanceCreationPassing',\n",
    " 'chanceCreationCrossing',\n",
    " 'chanceCreationShooting',\n",
    " 'defencePressure',\n",
    " 'defenceAggression',\n",
    " 'defenceTeamWidth',\n",
    " 'goals',\n",
    " 'pts',\n",
    " 'GF',\n",
    " 'GA',\n",
    " 'GD']\n",
    "num_columns=[ 'buildUpPlaySpeed',\n",
    " 'buildUpPlayDribbling',\n",
    " 'buildUpPlayPassing',\n",
    " 'chanceCreationPassing',\n",
    " 'chanceCreationCrossing',\n",
    " 'chanceCreationShooting',\n",
    " 'defencePressure',\n",
    " 'defenceAggression',\n",
    " 'defenceTeamWidth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match[req_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_class = (\n",
    "    melted_match.groupby(['season', 'league_name', 'Team'] + num_columns, as_index=False) \n",
    "    .agg({\n",
    "        'match_api_id': 'count',  # Total matches played\n",
    "        'pts': [\n",
    "            lambda x: (x == 3).sum(),  # Wins\n",
    "            lambda x: (x == 1).sum(),  # Draws\n",
    "            lambda x: (x == 0).sum(),  # Losses\n",
    "            'sum'  # Total points\n",
    "        ],\n",
    "        'GF': 'sum',\n",
    "        'GA': 'sum',\n",
    "        'GD': 'sum',\n",
    "        'goals': 'mean',  # Average goals\n",
    "    })\n",
    ")\n",
    "\n",
    "# Flatten the column names\n",
    "melted_class.columns = [\n",
    "    'season', 'league_name', 'Team'] + num_columns + ['MP', \n",
    "    'W', 'D', 'L', 'Pts', 'GF', 'GA', 'GD', 'Avg_Goals']\n",
    "\n",
    "# Sort the result by total_pts\n",
    "melted_class = melted_class.sort_values(by=['season', 'league_name', 'Pts'], ascending=[True, True, False])\n",
    "\n",
    "melted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the numeric columns to ensure equal weighting during clustering.\n",
    "- Use the relevant columns for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for analysis\n",
    "numeric_columns = [\n",
    "    'buildUpPlaySpeed', 'buildUpPlayDribbling', 'buildUpPlayPassing', \n",
    "    'chanceCreationPassing', 'chanceCreationCrossing', 'chanceCreationShooting',\n",
    "    'defencePressure', 'defenceAggression', 'defenceTeamWidth'\n",
    "]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "melted_class_normalized = melted_class.copy()\n",
    "melted_class_normalized[numeric_columns] = scaler.fit_transform(melted_class[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "num_clusters = 4\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "melted_class_normalized['Cluster'] = kmeans.fit_predict(melted_class_normalized[numeric_columns])\n",
    "\n",
    "# Add cluster labels to the original data\n",
    "melted_class['Cluster'] = melted_class_normalized['Cluster']\n",
    "\n",
    "# Display the final dataframe\n",
    "melted_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing Clusters\n",
    "- Calculate the average values of numeric columns for each cluster to interpret playing styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analysis = (\n",
    "    melted_class\n",
    "    .groupby('Cluster')[numeric_columns]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cluster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cluster labels\n",
    "cluster_labels = {\n",
    "    0: 'Balanced',\n",
    "    1: 'Balanced-to-Offensive',\n",
    "    2: 'Defensive',\n",
    "    3: 'Offensive'  \n",
    "}\n",
    "\n",
    "# Rename clusters\n",
    "cluster_analysis['Cluster'] = cluster_analysis['Cluster'].map(cluster_labels)\n",
    "cluster_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radar Chart Visualization\n",
    "- Visualize the playing styles for each cluster using radar charts\n",
    "- Offensive Playstyle: High buildUpPlayPassing, chanceCreationPassing, chanceCreationShooting. Low defencePressure and defenceAggression.\n",
    "- Defensive Playstyle: High defencePressure, defenceAggression, and defenceTeamWidth. Low offensive attributes.\n",
    "- Balanced Playstyle: Moderate values across both offensive and defensive attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes\n",
    "attributes = [\n",
    "    'buildUpPlaySpeed', 'buildUpPlayDribbling', 'buildUpPlayPassing',\n",
    "    'chanceCreationPassing', 'chanceCreationCrossing', 'chanceCreationShooting',\n",
    "    'defencePressure', 'defenceAggression', 'defenceTeamWidth'\n",
    "]\n",
    "\n",
    "# Angles for radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(attributes), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Close the circle\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_label in cluster_analysis['Cluster'].unique():\n",
    "    # Filter data for the current cluster\n",
    "    cluster_data = cluster_analysis[cluster_analysis['Cluster'] == cluster_label]\n",
    "    \n",
    "    # Extract values\n",
    "    cluster_values = cluster_data[attributes].values[0].tolist()\n",
    "    cluster_values += cluster_values[:1]\n",
    "\n",
    "    # Plot \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.polar(angles, cluster_values, marker='o', color='blue', label=cluster_label)\n",
    "    plt.fill(angles, cluster_values, color='blue', alpha=0.25)\n",
    "    \n",
    "    # Add labels\n",
    "    plt.thetagrids(np.degrees(angles[:-1]), labels=attributes, fontsize=10)\n",
    "    plt.title(f'{cluster_label} Cluster Playing Style', size=14, color='blue', pad=20)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.2))\n",
    "\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Comparing Clusters with Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clusters with performance metrics\n",
    "performance_analysis = melted_class[['Team', 'Cluster', 'Pts', 'GF', 'GA', 'GD']]\n",
    "\n",
    "# Group by cluster and analyze average performance\n",
    "performance_by_cluster = (\n",
    "    performance_analysis\n",
    "    .groupby('Cluster')[['Pts', 'GF', 'GA', 'GD']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "performance_by_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cluster labels\n",
    "cluster_labels = {\n",
    "    0: 'Offensive',\n",
    "    1: 'Balanced',\n",
    "    2: 'Defensive-to-Balanced',\n",
    "    3: 'Weak Defensive'  \n",
    "}\n",
    "\n",
    "# Rename the clusters\n",
    "performance_by_cluster['Cluster'] = performance_by_cluster['Cluster'].map(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize performance differences using bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.barplot(data=performance_by_cluster.melt(id_vars='Cluster'), x='Cluster', y='value', hue='variable')\n",
    "\n",
    "plt.title('Performance Metrics by Cluster', fontsize=16)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Value')\n",
    "plt.legend(title='Metric')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "- Radar charts helps to identify the offensive, defensive, or balanced styles of each cluster.\n",
    "- From the performance metrics, offensive and balanced styles are the most effective playing styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 2: Analysisng the correlation of Attributes with Performance\n",
    "- Determine how specific attributes impact performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for analysis\n",
    "numeric_columns = ['buildUpPlaySpeed', 'buildUpPlayDribbling', 'buildUpPlayPassing', \n",
    "                   'chanceCreationPassing', 'chanceCreationCrossing', 'chanceCreationShooting', \n",
    "                   'defencePressure', 'defenceAggression', 'defenceTeamWidth', \n",
    "                   'GF', 'GA', 'GD', 'pts', 'goals']\n",
    "\n",
    "# Filter relevant columns\n",
    "correlation_data = melted_match[numeric_columns]\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "correlation_matrix # table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the correlation matrix in a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='cividis_r', cbar=True)\n",
    "plt.title('Correlation Between Class Attributes and Performance Metrics')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the correlation Between Build-Up Play Passing and Goals Scored on a Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=melted_class, x='buildUpPlayPassing', y='GF')\n",
    "\n",
    "plt.title('Correlation Between Build-Up Play Passing and Goals Scored (GF)')\n",
    "plt.xlabel('Build-Up Play Passing')\n",
    "plt.ylabel('Goals Scored (GF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression for buildUpPlayPassing and GF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = melted_class[['buildUpPlayPassing']]\n",
    "y = melted_class['GF']\n",
    "\n",
    "# Add a constant for the regression\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display results\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 3: League-Level Style Analysis\n",
    "- Assess the dominant strategies in different leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style-related columns\n",
    "style_columns = [\n",
    "    'buildUpPlaySpeed', 'buildUpPlayDribbling', 'buildUpPlayPassing', \n",
    "    'chanceCreationPassing', 'chanceCreationCrossing', 'chanceCreationShooting', \n",
    "    'defencePressure', 'defenceAggression', 'defenceTeamWidth'\n",
    "]\n",
    "\n",
    "# Group by league and compute mean for style-related columns\n",
    "league_style_analysis = melted_match.groupby('season','league_name')[style_columns].mean().reset_index(\n",
    "\n",
    "league_style_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the data\n",
    "league_style_melted = league_style_analysis.melt(\n",
    "    id_vars='league_name', \n",
    "    var_name='Style Attribute', \n",
    "    value_name='Average Value'\n",
    ")\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=league_style_melted, y='Average Value', x='Style Attribute', hue='league_name', palette='Paired')\n",
    "\n",
    "plt.title('Average Playing Styles Across Leagues', fontsize=16)\n",
    "plt.xlabel('Style Attribute', fontsize=12)\n",
    "plt.ylabel( 'Average Value', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend(title='League', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10) \n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(league_style_analysis.set_index('league_name'), annot=True, fmt='.2f', cmap='viridis')\n",
    "\n",
    "plt.title('Heatmap of Average Playing Styles by League', fontsize=16)\n",
    "plt.xlabel('Style Attribute', fontsize=12)\n",
    "plt.ylabel('League', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Compare the average values for specific attributes like buildUpPlaySpeed or defenceAggression across leagues.\n",
    "For instance, the Premier League might have higher buildUpPlaySpeed, indicating a faster playing style.\n",
    "Heatmap Insights:\n",
    "\n",
    "Provides a comprehensive view of how leagues differ in their average style attributes.\n",
    "Highlights which leagues focus more on specific aspects (e.g., chanceCreationShooting or defenceTeamWidth).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 4  Which players had the most penalties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to filter and obtain the player who scored most of the penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for numeric columns\n",
    "numeric_columns = ['height', 'weight', 'overall_rating', 'potential']\n",
    "summary_stats = player_info[numeric_columns].describe().T\n",
    "summary_stats['median'] = player_info[numeric_columns].median()\n",
    "summary_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each category in 'preferred_foot'\n",
    "foot_distribution = player_info['preferred_foot'].value_counts()\n",
    "\n",
    "# Plot a bar chart\n",
    "sns.barplot(x=foot_distribution.index, y=foot_distribution.values, palette='pastel')\n",
    "plt.title('Distribution of Preferred Foot', fontsize=16)\n",
    "plt.xlabel('Preferred Foot', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences for attacking and defensive work rates\n",
    "attacking_work_rate_dist = player_info['attacking_work_rate'].value_counts()\n",
    "defensive_work_rate_dist = player_info['defensive_work_rate'].value_counts()\n",
    "\n",
    "print(\"Attacking Work Rate Distribution:\")\n",
    "print(attacking_work_rate_dist)\n",
    "\n",
    "print(\"\\nDefensive Work Rate Distribution:\")\n",
    "print(defensive_work_rate_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two distributions for visualization\n",
    "work_rate_data = pd.DataFrame({\n",
    "    'Attacking Work Rate': attacking_work_rate_dist,\n",
    "    'Defensive Work Rate': defensive_work_rate_dist\n",
    "}).T\n",
    "\n",
    "# Create a grouped bar chart\n",
    "work_rate_data.plot(kind='bar', figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "plt.title('Distribution of Work Rates', fontsize=16)\n",
    "plt.xlabel('Work Rate Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Work Rate Level', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' column is in datetime format\n",
    "player_info['date'] = pd.to_datetime(player_info['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To track trends over time, you can group the data by date and calculate the average of the attributes (overall_rating, dribbling, crossing, long_passing).\n",
    "\n",
    "# Group by 'date' and calculate the average for selected columns\n",
    "attributes = ['overall_rating', 'dribbling', 'crossing', 'long_passing']\n",
    "trend_data = player_info.groupby('date')[attributes].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trends\n",
    "plt.figure(figsize=(12, 8))\n",
    "for attribute in attributes:\n",
    "    sns.lineplot(data=trend_data, x='date', y=attribute, label=attribute)\n",
    "\n",
    "plt.title('Trend Analysis of Player Attributes Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Value', fontsize=12)\n",
    "plt.legend(title='Attributes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data points are too granular, you can resample the data to a monthly or yearly frequency to smooth out the trends.\n",
    "# Resample to monthly averages (if there are multiple records per month)\n",
    "trend_data_monthly = trend_data.set_index('date').resample('M')[attributes].mean().reset_index()\n",
    "\n",
    "# Plotting the resampled data\n",
    "plt.figure(figsize=(12, 8))\n",
    "for attribute in attributes:\n",
    "    sns.lineplot(data=trend_data_monthly, x='date', y=attribute, label=attribute)\n",
    "\n",
    "plt.title('Monthly Trend Analysis of Player Attributes', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Value', fontsize=12)\n",
    "plt.legend(title='Attributes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Player Development\n",
    "# To track individual player development, you can follow a similar approach but group by player_name or player_api_id \n",
    "# and aggregate data for each player over time\n",
    "# Group by 'date' and 'player_name' to track individual player trends\n",
    "player_trends = player_info.groupby(['date', 'player_name'])[attributes].mean().reset_index()\n",
    "\n",
    "# Plot for one specific player (e.g., a player named \"Lionel Messi\")\n",
    "player_trends[player_trends['player_name'] == 'Lionel Messi'].plot(x='date', y=attributes, figsize=(12, 8), marker='o')\n",
    "plt.title('Player Development for Lionel Messi', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Attribute Value', fontsize=12)\n",
    "plt.legend(title='Attributes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player Age and Potential\n",
    "Use birthday to calculate player age and analyze:\n",
    "Age distribution.\n",
    "Relationship between age and potential or overall_rating.\n",
    "Peak performance age for players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'birthday' column is in datetime format\n",
    "player_info['birthday'] = pd.to_datetime(player_info['birthday'])\n",
    "\n",
    "# Calculate player's age\n",
    "player_info['age'] = (pd.to_datetime('today') - player_info['birthday']).astype('<m8[Y]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot age distribution (Histogram)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(player_info['age'], kde=True, bins=20, color='skyblue')\n",
    "plt.title('Age Distribution of Players', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship Between Age and Potential / Overall Rating\n",
    "You can use scatter plots to examine how age relates to potential and overall_rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age vs. Potential\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=player_info, x='age', y='potential', color='orange')\n",
    "plt.title('Age vs Potential', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Potential', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age vs. Overall Rating\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=player_info, x='age', y='overall_rating', color='green')\n",
    "plt.title('Age vs Overall Rating', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Overall Rating', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak Performance Age for Players\n",
    "To find the peak performance age, you can aggregate the data by age and calculate the average of potential and overall_rating. Then, you can identify the age with the highest average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by age and calculate the mean of potential and overall_rating\n",
    "age_performance = player_info.groupby('age')[['potential', 'overall_rating']].mean().reset_index()\n",
    "\n",
    "# Plot Age vs Average Ratings\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=age_performance, x='age', y='potential', label='Potential', color='orange', linewidth=2)\n",
    "sns.lineplot(data=age_performance, x='age', y='overall_rating', label='Overall Rating', color='green', linewidth=2)\n",
    "\n",
    "# Highlight peak performance ages\n",
    "peak_performance_age = age_performance.loc[age_performance['overall_rating'].idxmax(), 'age']\n",
    "plt.axvline(x=peak_performance_age, color='red', linestyle='--', label=f'Peak Performance Age: {peak_performance_age}')\n",
    "\n",
    "plt.title('Age vs Average Potential and Overall Rating', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Average Rating', fontsize=12)\n",
    "plt.legend(title='Attributes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform Attribute Clustering using K-Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Select relevant features (example)\n",
    "attributes = ['sprint_speed', 'crossing', 'vision', 'short_passing', 'dribbling', 'ball_control']\n",
    "\n",
    "# Subset the data\n",
    "player_attributes = player_info[attributes]\n",
    "\n",
    "# Standardize the data (important for clustering)\n",
    "scaler = StandardScaler()\n",
    "scaled_attributes = scaler.fit_transform(player_attributes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "player_info['cluster'] = kmeans.fit_predict(scaled_attributes)\n",
    "\n",
    "# Check the cluster centers (means of the attributes for each cluster)\n",
    "cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=attributes)\n",
    "print(cluster_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the archetypes based on the cluster centers\n",
    "archetypes = {\n",
    "    0: 'Speedy Winger',\n",
    "    1: 'Playmaker',\n",
    "    2: 'All-rounder'\n",
    "}\n",
    "\n",
    "# Map the cluster labels to archetypes\n",
    "player_info['archetype'] = player_info['cluster'].map(archetypes)\n",
    "\n",
    "# Show players with their archetypes\n",
    "print(player_info[['player_name', 'archetype']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For 2D visualization, we can use the first two principal components (PCA) of the scaled data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_attributes)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pca_df['archetype'] = player_info['archetype']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='archetype', palette='Set1', s=100)\n",
    "plt.title('Player Archetypes Based on Skills', fontsize=16)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.legend(title='Archetype', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The soccer database has five datasets, league, country, player, player attribute, team and team attribute. It is a detailed dabase for European major leagues covering several seasons from 2008/2009 t0 2015/2016. \n",
    "\n",
    "The project seeks to answer three questions, what teams improved the most over the time period, which players had the most penalties and which was the the most preferred leg for penalty-takers in 2016 among the players who scored more than the mean penalties in that year?\n",
    "\n",
    "In attempting to find solutions to the question, each dataset was examineed for inconsistencies, colomn names, corrected, missing values replace or droped in certain datasets before they were finally merged and cleaned. Visual presentations created and inteprated.\n",
    "\n",
    "From the analysis and visualization, Richie Lambert is the player who scored most of the penalties. I also found that Paris Saint-Germain is the most improved team over the period of time given, followed by Napoli and Cracovia being the in the third position. Moreover, the findings also indicate that most of the penalty takers in 2016 preferred right leg compared to the right leg. The findings also shows that the distribution of the number of goals scored in the two seasons are right skewed.\n",
    "\n",
    "Whereas I was able to show that there is a correltaion between the number of goals scored in the two extreme seasons (2008/2009 and 2015/2016), theer are  other seasons that were not considered. There is likelihood that a team that improved between the two seasons might not have improved in the seasons prior 2015/2016. Goal difference between the two seasons was used as a measured of improvement in performance because the ultimate objective of team managers, players and teams is to improve to score goals, but there could be criteria for measuring performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from subprocess import call\n",
    "#call(['python', '-m', 'nbconvert', 'Investigate_a_Dataset.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
