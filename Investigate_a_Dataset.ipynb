{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Investigate a Dataset - [Database_soccer]\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "### Dataset Description \n",
    "\n",
    "> In this project 1, I willl be analysing ultimate soccer dataset, which is an open-source dataset in kaggle. The dataset is a one .sql file comprising seven tables, each with different(unique) but interrelated features. First, Country table has 11 European countries. Second, league table has 11 lead championship names. The country and league tables are related by their ID. Third, match table has over 25, 000 matches for different seasons as well as betting odds from upto 10 providers. The match table is also related to the previous tables by country_id. in the rows and 2 columns id and name\n",
    "I have check the shape of table to determine the nummber of rows and columns.\n",
    "\n",
    "\n",
    "### Question(s) for Analysis\n",
    "1. What teams improved the most over the time period? \n",
    "2. Which players had the most penalties? \n",
    "3. Which was the the most preferred leg for penalty-takers in 2016 among the players who scored more than the mean penalties in that year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for all of the packages to be used.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to create a function that would read csv and load for very dataset to a name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to load data\n",
    "path='C:/Users/Davie/Desktop/Data/'\n",
    "def load_data(name, table_name):\n",
    "    name=pd.read_csv(path + 'Database_Soccer/'+ table_name) # reads the csv file and stores in the dataframe name\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "displaying few lines of each dataset from the soccer database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=load_data('country', 'Country.csv')# country data table\n",
    "country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league=load_data('league', 'League.csv')# league data table\n",
    "league.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the league name above, the league name for Germany is confusing. Germany 1. could mean there are a number of German countries, so it should be change to Germany Bundesliga 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match=load_data('match', 'Match.csv')# match data table\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing data in match table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player=load_data('player', 'Player.csv')# player data table\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The birthday contains time at 00:00:00, which could be removed to contain only year, month and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_attr=load_data('player_attr', 'Player_Attributes.csv')# player attributes data table\n",
    "player_attr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time 00:00:00 can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team=load_data('team', 'Team.csv')# team data table\n",
    "team.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column team_fifa_api_id could insignificant because there is already team_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_attr=load_data('team_attr', 'Team_Attributes.csv')# team attributes data table\n",
    "team_attr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want again to develop a function that I will be using in inspecting the datasets for missing data, getting descriptive statistics, dimensions, and features' data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function for inspecting the datasets\n",
    "def wrangles (tbl_name):\n",
    "    \n",
    "    inf=tbl_name.info(); # inspecting data types and instances with missing data \n",
    "    dim=tbl_name.shape; # inspecting dimensions of the dataset\n",
    "    desc=tbl_name.describe(); # getting descriptive statistics\n",
    "        \n",
    "    return inf, dim, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country dataset\n",
    "wrangles(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need of getting the mean for ids, since the names are string we can just unique values and counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country.name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# league data\n",
    "wrangles(league)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need of getting the mean for ids, since the names are string we can just unique values and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league.name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match data\n",
    "wrangles(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing data for column home_player_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the date has been stored as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player data\n",
    "wrangles(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "birthday column stored as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player attributes data\n",
    "wrangles(player_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dates stored as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team data\n",
    "wrangles(team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data for team_fifa_api_id, though this might be insignificant since there is already team_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team attributes data\n",
    "wrangles(team_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data for buildUpPlayDribbling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Cleaning\n",
    "\n",
    "First, I want to create functions that will help me drop duplicates, merge two datasets, change data type, remove missing rows, drop unnecessary columns, then proceed to to merge the country data to that for league. \n",
    "I will correct the league name for Germany 1. Bundesliga to Germany Bundesliga 1.\n",
    "I will also change the name column for both the country data and league data, and also make the datafrmaes have the same dimensions and finally merge the two dataframes into country_league data using the country id as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to rename columns in a data frame\n",
    "def col_rename(col_renamed_data, col_old, col_new):\n",
    "    if len(col_old)==2: # checks if there are two columns to be renamed\n",
    "        col_renamed_data.rename(columns={col_old[0]:col_new[0], col_old[1]:col_new[1]}, inplace=True) # renames the two columns in the dataset\n",
    "    else:\n",
    "        col_renamed_data.rename(columns={col_old:col_new}, inplace=True) # renames if there is only one column to be renamed\n",
    "        \n",
    "    return col_renamed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function drop columns\n",
    "def drop_cols(drop_col_data, col_name):\n",
    "    drop_col_data.drop(col_name, axis=1, inplace=True) # removing columns\n",
    "        \n",
    "    return drop_col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to remove row missing values\n",
    "def remove_row_missing_values(na_data):\n",
    "    na_data.dropna(axis=0, how='any', inplace=True) # removing all rows with missing values\n",
    "        \n",
    "    return na_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to remove duplicate rows\n",
    "def remove_duplicates(dup_data, col):\n",
    "    if col=='':\n",
    "        dup_data.drop_duplicates(inplace=True) # remove all duplicate rows\n",
    "        \n",
    "    else:\n",
    "        dup_data.dropna(subset=[col], inplace=True) # removing rows based on column duplicate values\n",
    "        \n",
    "    return dup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to change date column from string to datetime\n",
    "def changed_type(changed_type_data, col_type):\n",
    "    changed_type_data[col_type]=changed_type_data[col_type].astype('str')  # converting to string\n",
    "    changed_type_data[col_type]=changed_type_data[col_type].str.extract(r'(\\d{4}-\\d{2}-\\d{2})') # extracting the date\n",
    "    changed_type_data[col_type] = pd.to_datetime(changed_type_data[col_type], format='%Y-%m-%d') # converting to datetime\n",
    "\n",
    "    return changed_type_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to filter some columns\n",
    "def filter_col(f_data, col):\n",
    "    df=f_data.filter(col) # filters the columns\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to to merge two data frames using inner because i dont want do keep duplicates\n",
    "def merging_data(data1, data2, on_col):\n",
    "    df=data1.merge(data2, on =on_col, how='inner')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting the league name for Germany 1. Bundesliga\n",
    "league.replace(to_replace='Germany 1. Bundesliga', value='Germany Bundesliga 1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing both id and name columns for the country data\n",
    "country=col_rename(country, col_old=['name', 'id'], col_new=['country_name', 'country_id']) \n",
    "country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the name column and id column  for the league data\n",
    "league=col_rename(league, col_old=['name', 'id'], col_new=['league_name', 'league_id']) \n",
    "league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two dataframes country and league\n",
    "country_league_info=merging_data(country, league, on_col ='country_id')\n",
    "country_league_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I want to merge the player data to player attributes data into player_info dataframe.\n",
    "I will use either the player_api_id or player_fifa_api_id as the keys and drop the id columns in both datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of player_api_id on payer and player attribute data\n",
    "player.player_api_id.nunique()==player_attr.player_api_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of  player_fifa_api_id on player and player attribute data\n",
    "player.player_fifa_api_id.nunique()==player_attr.player_fifa_api_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of unique values of the player_fifa_api_id are not the same in both dataframes, I will just use both of them as the key because both the ids might be important in merging this data with another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping id column from player dataframe\n",
    "drop_cols(player, col_name='id')\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping id column from player_attr dataframe\n",
    "drop_cols(player_attr, col_name='id')\n",
    "player_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates for player attribute data\n",
    "remove_duplicates(player_attr, col='')\n",
    "player_attr.sort_values(['player_api_id', 'date']) # sorting the players by id and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the two dataframes on player_api_id and player_fifa_api_id using inner join because i dont want the unmatched rows\n",
    "player_info=merging_data(player, player_attr, on_col =['player_api_id', 'player_fifa_api_id'])\n",
    "player_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now want o examine the player_info dataframe, check for missing values, dimensions of each column, data types of each column as well as duplcate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above uotput, there are several missing values, the data type for birthday and date are all strings. I have also noted that attacking_work_rate has the least number of rows hence maximum number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the uniques for the coulmn attacking_work_rate\n",
    "player_info.attacking_work_rate.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to drop all rows with nan and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all rows with missing data\n",
    "remove_row_missing_values(player_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicate rows\n",
    "remove_duplicates(player_info, col='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.attacking_work_rate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.defensive_work_rate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output in above two cells, no information has been provided from the data description about the meaning of None, norm, y, stoc, le, ornal, es, tocky, ean o, and the numbers 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9 . Howerver, from inspection, I realize that there is some association in that all the numbers 0-9 and o on the defensive_work_rate relates to None  on the attacking_work_rate. Also, the following  pairs also relate: norm-ornal, y-es, stoc-tocky, le-ean. These could be change or transformed if additional information is provided or simply be dropped from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacking_none=player_info.query('attacking_work_rate==\"None\"')\n",
    "attacking_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing length of numbers 0-9 and o in the defensive_work_rate relates to None in the attacking_work_rate\n",
    "\n",
    "attacking_none.attacking_work_rate.value_counts()==attacking_none.defensive_work_rate.value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to change the data types for birthday and date from string to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting birthday column into datetime\n",
    "changed_type(player_info, col_type='birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date column into datetime\n",
    "changed_type(player_info, col_type='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, I want merge team data to team attributes data into into team info dataframe, check data types, missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of team_api_id in team and team attribute data\n",
    "team.team_api_id.nunique()==team_attr.team_api_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values of team_fifa_api_id in team and team attribute data\n",
    "team.team_fifa_api_id.nunique()==team_attr.team_fifa_api_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these outputs, it is clear that neither the team_api_id nor the team_fifa_api_id matches in the two datasets. I will therefore merge them on both the team_api_id and team_fifa_api_id as the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the id for team dataframe\n",
    "drop_cols(team, col_name='id').drop_duplicates(subset='team_api_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team.team_long_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to correct the following team names: '1. FC Köln', '1. FC Nürnberg', '1. FSV Mainz 05','1. FC Kaiserslautern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting some team names\n",
    "team.team_long_name.replace(to_replace=['1. FC Köln', '1. FC Nürnberg', '1. FSV Mainz 05','1. FC Kaiserslautern'], \n",
    "                            value=['FC Köln', 'FC Nürnberg', 'FSV Mainz 05','FC Kaiserslautern'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_attr.dropna(how='any').drop_duplicates(subset='team_api_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the id for team attributes data\n",
    "drop_cols(team_attr, col_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging team to the team attribute\n",
    "team_info=team.merge(team_attr, on =['team_api_id', 'team_fifa_api_id'], how='inner') #\n",
    "team_info['team_fifa_api_id']=team_info['team_fifa_api_id'].astype(int)\n",
    "team_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will populate the NaN in the buildUpPlayDribbling column with the mean of the column. And finally drop duplcates in the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the mean of buildUpPlayDribbling column\n",
    "#mean=team_info.buildUpPlayDribbling.mean()\n",
    "\n",
    "# filling the NaNs in the buildUpPlayDribbling by the mean\n",
    "#team_info['buildUpPlayDribbling']=team_info['buildUpPlayDribbling'].fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime\n",
    "changed_type(team_info, col_type='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicate rows in the final team info merged data\n",
    "drop_cols(team_info, col_name=['team_fifa_api_id',]).drop_duplicates(subset='team_api_id').dropna(how='any', inplace=True)\n",
    "#remove_duplicates(team_info, col='')\n",
    "team_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I to examine and merge match data, country_league, and team data to form march info dataframe. I will check for duplicates, missing values and correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of league_id unique values in the match and country_league_info dataframes\n",
    "match.league_id.nunique()==country_league_info.league_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of country_id unique values in the match and country_league_info dataframes\n",
    "match.country_id.nunique()==country_league_info.country_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will merge match and country_league_info dataframes on league_id and country_id, so that I retain all the info about ids. I want to drop all the columns containing odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in match.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to remove all the columns with CAPITAL LETTERS, odds columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime for the match data\n",
    "changed_type(match, col_type='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicate rows from match data\n",
    "remove_duplicates(match, col='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns containg odds and the match id\n",
    "odd_cols = list(filter(lambda x: x.isupper(), match.columns))\n",
    "drop_cols(match, col_name=odd_cols)\n",
    "drop_cols(match, col_name='id')\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the contents of some columns\n",
    "```match.goal.value_counts() # goal```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two cells, I realised that the coulmns goal, shoton, shotoff, foulcommit, card, cross, corner and possession contains information related to the web page but not realistic data.Checking through the nested infomation, I can't really make sense out of it since even the website link to the discription is not loading. Instead of deleting the columns with such issues, I will instead drop the columns containing unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.columns\n",
    "match_info=filter_col(match, col=['country_id', 'league_id', 'season', 'stage', 'date', 'match_api_id', 'home_team_api_id', 'away_team_api_id', 'home_team_goal', 'away_team_goal'])\n",
    "\n",
    "match_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_info['results'] = np.where(\n",
    "    match_info['home_team_goal'] == match_info['away_team_goal'], 'draw',\n",
    "    np.where(\n",
    "        match_info['home_team_goal'] > match_info['away_team_goal'], 'home win', 'away win'\n",
    "    )\n",
    ")\n",
    "\n",
    "match_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging for ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_info=merging_data(country_league_info, match_info, on_col =['country_id', 'league_id'])\n",
    "match_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_info=match_info.drop(columns=['country_id', 'league_id'])\n",
    "match_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt home and away teams id_vars are columns not melted\n",
    "# creating columns not melted\n",
    "unmelted_columns= [x for x in match_info.columns if x not in ['home_team_api_id', 'away_team_api_id']]\n",
    "\n",
    "# melting home and away teams api ids\n",
    "melted_match=match_info.melt(id_vars=unmelted_columns, var_name='Location', value_name='Team')\n",
    "melted_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up or changing the location values\n",
    "melted_match['Location']=melted_match['Location'].replace({'home_team_api_id':'home', 'away_team_api_id':'away'})\n",
    "melted_df=melted_match.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging the team_info to the melted_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['team_fifa_api_id', 'team_long_name', 'team_short_name', 'date']\n",
    "team_info = team_info.drop(columns=exclude_columns).drop_duplicates(subset='team_api_id')\n",
    "\n",
    "team_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info.rename(columns={'team_api_id':'Team'}, inplace=True)\n",
    "team_s=team_info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "melted_match=pd.merge(melted_df, team_s, on ='Team', how='left')\n",
    "#melted_match.drop_duplicates(subset='match_api_id' , inplace=True)\n",
    "melted_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating team dictionary with team api ids as the key and a longname as the value\n",
    "team_dict=team.set_index('team_api_id')['team_long_name'].to_dict()\n",
    "team_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up team i.e. using team dictionary to replace team api ids\n",
    "melted_match['Team']=melted_match['Team'].map(team_dict)\n",
    "melted_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating goals column\n",
    "melted_match['goals']=np.where(melted_match['Location']=='home',melted_match['home_team_goal'], melted_match['away_team_goal'])\n",
    "melted_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match['pts'] = np.where(\n",
    "    melted_match['results'] == 'draw', 1, \n",
    "    np.where(\n",
    "        (melted_match['results'] == 'away win') & (melted_match['Location'] == 'home'), 0, \n",
    "        np.where(\n",
    "            (melted_match['results'] == 'home win') & (melted_match['Location'] == 'away'), 0, 3\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "melted_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols(melted_match, col_name=['home_team_goal', 'away_team_goal'])\n",
    "melted_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match.sort_values(by=['match_api_id', 'season'])\n",
    "melted_match.drop_duplicates() #subset='match_api_id', inplace=True\n",
    "melted_match.dropna(how='any')\n",
    "melted_match.sort_values(by='match_api_id', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a function to plot different visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_visual(data, data2, visual_type):\n",
    "    if visual_type=='barh':\n",
    "        data.plot(kind='barh', rot=0, width=0.7, alpha=0.8, color='grey', figsize=[8,20] ) # i want to creat horizontal bars\n",
    "            \n",
    "    elif visual_type=='hist':\n",
    "        fig,vis=plt.subplots(figsize=[10,8])\n",
    "        vis.hist(data, alpha=0.8) # creating a histogram\n",
    "        #plt.grid(axis='y', alpha=0.6) # grid\n",
    "            \n",
    "    elif visual_type=='boxplot':\n",
    "        fig,vis=plt.subplots(figsize=[8,4])\n",
    "        vis.boxplot(data, vert=0) # creating a box plot\n",
    "        #plt.grid(axis='x', alpha=0.6)\n",
    "                     \n",
    "    else:\n",
    "        fig, vis=plt.subplots(figsize=[10,8])\n",
    "        plt.scatter(x=data, y=data2, alpha=0.8, color='blue') # creating scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 1 What teams improved the most over the time period? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_match['season'].unique("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_08_16 = (\n",
    "    melted_match\n",
    "    .query('season == \"2015/2016\" and league_name == \"England Premier League\"')  # Filter for the specified season and league\n",
    "    .groupby(['season', 'league_name', 'Team'], as_index=False)\n",
    "    .agg({'pts': 'sum', 'goals': 'mean'})  # Sum 'pts' and calculate mean of 'goals'\n",
    "    .sort_values(by=['season', 'league_name', 'pts'], ascending=[True, True, False])  # Sort by season, league, then pts\n",
    ")\n",
    "\n",
    "goal_08_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=goal_08_16, x='Team', y='pts', hue='league_name', dodge=False)\n",
    "plt.title('Total Points by Team within Leagues')\n",
    "plt.xlabel('Team')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for heatmap\n",
    "heatmap_data = goal_08_16.pivot('Team', 'league_name', 'pts')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.1f', linewidths=0.5)\n",
    "plt.title('Points Heatmap by Team and League')\n",
    "plt.xlabel('League Name')\n",
    "plt.ylabel('Team')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for a specific league and season\n",
    "league_data = goal_08_16[goal_08_16['league_name'] == 'England Premier League']\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(league_data['pts'], labels=league_data['Team'], autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Points Contribution by Teams in League A (2015/2016)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "goal_08_16.sort_values('pts', ascending=True).plot(\n",
    "    kind='barh', x='Team', y='pts', color='skyblue', legend=False\n",
    ")\n",
    "plt.title('Total Points by Team')\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Team')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another column (e.g., number of matches)\n",
    "goal_08_16['matches'] = goal_08_16['pts'] // 3  # Assuming 3 pts per match\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(goal_08_16['Team'], goal_08_16['pts'], s=goal_08_16['matches'] * 20, alpha=0.6, edgecolors=\"w\")\n",
    "plt.title('Bubble Chart: Points vs Matches Played')\n",
    "plt.xlabel('Team')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart for each league\n",
    "plt.figure(figsize=(10, 6))\n",
    "for league in goal_08_16['league_name'].unique():\n",
    "    league_data = goal_08_16[goal_08_16['league_name'] == league]\n",
    "    plt.plot(league_data['Team'], league_data['pts'], marker='o', label=league)\n",
    "\n",
    "plt.title('Points Trends Across Teams by League')\n",
    "plt.xlabel('Team')\n",
    "plt.ylabel('Points')\n",
    "plt.legend(title='League')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=goal_08_16, x='Team', y='pts', hue='season', kind='bar', height=6, aspect=1.5)\n",
    "plt.title('Total Points by Team and Season')\n",
    "plt.xlabel('Team')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_teams = (\n",
    "    melted_match\n",
    "    .query('season in [\"2015/2016\"]')  # Filter for the specified season\n",
    "    .groupby(['season', 'league_name', 'Team'], as_index=False)['pts']\n",
    "    .sum()  # Sum points for each team\n",
    "    .sort_values(by=['season', 'league_name', 'pts'], ascending=[True, True, False])  # Sort by season, league, then pts\n",
    "    .groupby(['season', 'league_name'], as_index=False)  # Group by season and league\n",
    "    .head(3)  # Keep only the top 3 rows for each group\n",
    ")\n",
    "\n",
    "top_3_teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_08_16 = (\n",
    "    melted_match\n",
    "    .query('season in [\"2015/2016\"]')  # Filter for the desired season\n",
    "    .groupby(['season', 'league_name', 'Team'])['pts']  # Group by season, league, and team\n",
    "    .sum()  # Sum the points for each team\n",
    "    .reset_index()  # Reset index to make the grouped columns part of the DataFrame\n",
    "    .sort_values(by='pts', ascending=False)  # Sort by points in descending order\n",
    ")\n",
    "\n",
    "goal_08_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of the extreme seasons\n",
    "goal_08_16=melted_match.query('season in [\"2008/2009\", \"2015/2016\"]').groupby(['season', 'Team'])['goals'].sum()\n",
    "# plot histogram\n",
    "plot_visual(data=goal_08_16, data2='', visual_type='hist')\n",
    "plt.title('SEASON 2008/2009 AND 2015/2016 HISTOGRAM')\n",
    "plt.xlabel('bins')\n",
    "plt.ylabel('No of goals scored in 2008/2009 and 2015/2015 season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that the distribution of goals is right screwed. Further investigation can be shown on the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot box plot\n",
    "plot_visual(data=goal_08_16, data2='', visual_type='boxplot')\n",
    "plt.title('SEASON 2008/2009 AND 2015/2016 BOXPLOT')\n",
    "plt.xlabel('No of goals scored in 2008/2009 and 2015/2015 season')\n",
    "plt.ylabel('2008/2009 and 2015/2015 season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is rght skewed but with severa outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by season and team\n",
    "match_s08_15=melted_match.query('season in [\"2008/2009\", \"2015/2016\"]').groupby(['season', 'Team'])['goals'].sum().unstack('season')\n",
    "match_s08_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a scatter diagram\n",
    "plot_visual(data=match_s08_15['2008/2009'], data2=match_s08_15['2015/2016'], visual_type='scatter')\n",
    "plt.title('SCATTER PLOT FOR SEASON 2008/2016 VS 2015/2016')\n",
    "plt.xlabel('Season 2008/2009')\n",
    "plt.ylabel('Season 2015/2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, there is positive correlation between the number of goals scored in the season 2008/2009 and season 2015/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get difference between columns 2008/2009 and 2015/2016 seasons\n",
    "goal_diff=match_s08_15.dropna().diff(axis=1)\n",
    "goal_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal difference in 2015/2016 season\n",
    "goal_diff_16=goal_diff['2015/2016']\n",
    "goal_mean=goal_diff_16[goal_diff_16>(goal_diff_16.mean())].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar graph for top 10 best teams\n",
    "plot_visual(data=goal_mean, data2='', visual_type='barh')\n",
    "plt.title('SEASON 2008/2009 AND 2015/2016 BAR GRAPH')\n",
    "plt.xlabel('No of goals scored in 2008/2009 and 2015/2015 season')\n",
    "plt.ylabel('Teams');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, the top 3 most improved teams are Paris Saint_Germain, Napoli and Cracovia in that order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 2  Which players had the most penalties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to filter and obtain the player who scored most of the penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the name of the player\n",
    "most_pen=player_info.query('penalties ==penalties.max()')\n",
    "\n",
    "# creating dataframe for the player who had most penalties, avoiding duplicate name\n",
    "most_pen.drop_duplicates(subset=\"player_api_id\")\n",
    "\n",
    "# printing \n",
    "most_pen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows that the player that scored most of the penalties is Richie Lambert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 3  Which was the the most preferred leg for penalty-takers in 2016 among the players who scored more than the mean penalties in that year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting present in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pen16=player_info.query('date>=\"2016.01.01\"')\n",
    "most_pen16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the players who had penalties more than the mean penalties\n",
    "pen_mean=most_pen16[most_pen16.penalties>most_pen16.penalties.mean()]\n",
    "pen_mean.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping duplicates interms of player_api_id\n",
    "pen_mean.drop_duplicates(subset=['player_api_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pie chart for the preffered leg\n",
    "p=pen_mean['preferred_foot'].value_counts()\n",
    "p.plot('pie', figsize=[6,6])\n",
    "plt.title(\"PIE CHART FOR PREFfERRED LEG\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pie chart, most penalty takers in the year 2016 preffered right leg than left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Limitations\n",
    "The soccer database is a very extensive data. In, seeking to address the three questions in the description, I was able to show only the correltaion between the number of goals scored in the two season. However, the other seasons were not considered.The data base had a lot of unprocessed  html files under certain columns, which made take a lot of time thinking on how well they can be used in the analysis. In addition, the data had a lot of missing and duplicate values. Identifying such inconsistencies, wss realy time consuming.\n",
    "\n",
    "Another limitation is that in the creation of bar graph for the most improved teams, the bars are not sorted in order of either increasing or decreasing frequency, which would have enabled the identification of the improved teams easily. In getting the most preffered leg, it was only based on the year 2016 instead of the whole duration of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The soccer database has five datasets, league, country, player, player attribute, team and team attribute. It is a detailed dabase for European major leagues covering several seasons from 2008/2009 t0 2015/2016. \n",
    "\n",
    "The project seeks to answer three questions, what teams improved the most over the time period, which players had the most penalties and which was the the most preferred leg for penalty-takers in 2016 among the players who scored more than the mean penalties in that year?\n",
    "\n",
    "In attempting to find solutions to the question, each dataset was examineed for inconsistencies, colomn names, corrected, missing values replace or droped in certain datasets before they were finally merged and cleaned. Visual presentations created and inteprated.\n",
    "\n",
    "From the analysis and visualization, Richie Lambert is the player who scored most of the penalties. I also found that Paris Saint-Germain is the most improved team over the period of time given, followed by Napoli and Cracovia being the in the third position. Moreover, the findings also indicate that most of the penalty takers in 2016 preferred right leg compared to the right leg. The findings also shows that the distribution of the number of goals scored in the two seasons are right skewed.\n",
    "\n",
    "Whereas I was able to show that there is a correltaion between the number of goals scored in the two extreme seasons (2008/2009 and 2015/2016), theer are  other seasons that were not considered. There is likelihood that a team that improved between the two seasons might not have improved in the seasons prior 2015/2016. Goal difference between the two seasons was used as a measured of improvement in performance because the ultimate objective of team managers, players and teams is to improve to score goals, but there could be criteria for measuring performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from subprocess import call\n",
    "#call(['python', '-m', 'nbconvert', 'Investigate_a_Dataset.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
